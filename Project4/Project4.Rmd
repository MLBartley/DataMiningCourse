---
title: "Project 4"
output:
  html_notebook: default
  pdf_document:
    keep_tex: yes
    latex_engine: xelatex
  word_document: default
---


#Introduction

This is Project 4 for STAT 557 2018 Spring by Meridith Bartley and Fei Jiang. The aim of this project is to practice EM algorithm, MDA algrithm and compare MDA with QDA. 

#Description of Data
This dataset contains soil sample data over the US downloaded from Natural Resources Conservation Service (NRCS). After removing the incomplete data records and excluding the data records with impossible values, there are around 14,000 records left, each of which includes physical and chemical properties of soil samples (sand, silt, clay, organic carbon, bulk density, CEC soil, CEC clay, base saturation, and pH) and the corresponding soil classification group (soil order). 

Boxplots for each physical and chemical property used as explanitory variables in the subsequent classification models are included below. This EDA allows for early indication of which variables may possibly be ommitted during dimention reduction. That is, what properties do not differ significantly between soil Orders.


```{r load required packages, include=FALSE}
library(ggplot2)
library(dplyr)
library(magrittr)
library(MASS)
library(caret)
library(nnet) 
library(scales)
library(klaR)
library(stats)
library(grid)
library(gridExtra)
library(mclust)
library(mvtnorm)
library(mda)

```

```{r EM alg functions, include=FALSE}

### EM algorithm
# x:      covariates (as a matrix, one row = one observation)
# y:      response (as a factor)
# R:      number of subclasses for each class
# tol:    condition for convergence
# maxit:  maximum number of iterations for EM to run
EM <- function(x, y, R, tol = 1e-6, maxit = 10) {
  labs <- levels(y)
  K <- length(labs)
  M <- ncol(x)
  N <- nrow(x)
  pi <- array(0, dim = c(K, R))
  mu <- array(0, dim = c(K, M, R))
  sigma <- diag(M)
  y <- as.numeric(y)
  ### come up with starting points using kmeans
  for(k in 1:K) {
    ind <- which(y == k)
    mu[k, , ] <- t(kmeans(x[ind, ], R)$centers)
    pi[k, ] <- rep(1/R, length = R)
  }
  converged <- F
  ### main EM loop
  # while(!converged) {
  for(it in 1:maxit) {
    # E step
    p <- array(0, dim = c(N, R))
    for(i in 1:N) {
      k <- y[i]
      for(r in 1:R) {
        p[i,r] <- pi[k,r] * dmvnorm(x[i, ], mu[k, ,r], sigma)
      }
      p[i, ] <- p[i, ]/sum(p[i, ])
    }
    # M step
    for(k in 1:K) {
      ind <- which(y == k)
      for(r in 1:R) {
        pi[k,r] <- sum(p[ind,r])/length(ind)
        mu[k, ,r] <- colSums(x[ind, ]*p[ind,r])/sum(p[ind,r])
      }
    }
    tally <- 0
    for(i in 1:N) {
      z <- y[i]
      for(r in 1:R) {
        tmp <- x[i, ] - mu[z, ,r]
        tally <- tally + p[i,r] * (tmp %o% tmp)
      }
    }
    sigma <- tally/N
    # check convergence

  }
  return(list(pi = pi,
              mu = mu,
              sigma = sigma,
              class_labels = labs))
}

### predict classes
# em: object from the result of EM function above
# x: new data (as a matrix, one row = one observation)
em_predict <- function(em, x) {
  pi <- em$pi
  mu <- em$mu
  sigma <- em$sigma
  labs <- em$class_labels
  K <- dim(mu)[1]
  R <- dim(mu)[3]
  classes <- rep(NA, nrow(x))

  for(i in 1:nrow(x)) {
    probs <- rep(0, K)
    for(k in 1:K) {
      for(r in 1:R) {
        probs[k] <- probs[k] + pi[k,r] * dmvnorm(x[i, ], mean = mu[k, ,r], sigma = sigma)
      }
    }
    classes[i] <- which.max(probs)
  }
  return(factor(classes, labels = labs))
}
```

```{r EM applied to data, include=FALSE}

```


```{r load code and clean, include=FALSE, echo = F, message = F}
# Basic clearning of our dataset 
project1data <- read.csv(file = "project1data.csv")
project1data <- project1data[,-c(1,2,4)] #remove the ID and Horizon columns which are useless,Silt is redundant information since it could be calculated from Sand and Clay. 
project1data <- project1data[sample(nrow(project1data),nrow(project1data)),] ###shuffle rows


```

## Exploritory Data Analysis
```{r EDA - Mer, echo=FALSE, message = F, results = 'hide', fig.keep='all', fig.width = 8}

varlist <- names(project1data)[-9]

customPlot <- function(varName) {

project1data %>% 
group_by_("Order") %>% 
select_("Order",varName) %>% 
ggplot(aes_string("Order",varName)) + geom_boxplot() + 
   theme(axis.text.x = element_text(angle = 45, hjust = 1))

}

grid.arrange(grobs = lapply(varlist[1:4],customPlot))
grid.arrange(grobs = lapply(varlist[5:8],customPlot))


# pairs(project1data)
# cor(project1data[, -9])


```


#Analysis


# Mixture Discriminant Analysis (MDA)



```{r - Fei PCA, echo=FALSE}

# Create new dataset with PCA 
pca = prcomp(project1data[,-9])
# print(pca$rotation)
summary.pca = summary(pca)
# print(summary.pca)
project1data.new = data.frame(pca=pca$x[,1:8],Order=project1data$Order) 

#table of the variance and coefficients
#knitr::kable(summary.pca$importance)
#knitr::kable(pca$rotation[, 1:4])

```


```{r - Fei data partition, include=FALSE}

#data partition
train_id <- caret::createDataPartition(y=project1data.new$Order, p=0.8,list = FALSE)
train <- project1data.new[train_id,]
test <- project1data.new[-train_id,]
```


```{r MDA - Fei, echo =F}

#mda - 2 component 
mda.2 <- mda(Order ~ pca.PC1 + pca.PC2 , data = train)
#predict test data
mda2.predict = predict(mda.2, newdata=test)
# Assess the total accuracy of test data
ct2.mda <- table(test$Order, mda2.predict)
totcorrect2.mda <- sum(diag(prop.table(ct2.mda)))
# Assess the accuracy of the prediction for each soil class (Order) in test data
#cc2.mda <- diag(prop.table(ct2.mda, 1))

#mda - 3 component 
mda.3 <- mda(Order ~ pca.PC1 + pca.PC2+ pca.PC3 , data = train)
#predict test data
mda3.predict = predict(mda.3, newdata=test)
# Assess the total accuracy of test data
ct3.mda <- table(test$Order, mda3.predict)
totcorrect3.mda <- sum(diag(prop.table(ct3.mda)))

#mda - 4 component 
mda.4 <- mda(Order ~ pca.PC1 + pca.PC2+ pca.PC3+ pca.PC4 , data = train)
#predict test data
mda4.predict = predict(mda.4, newdata=test)
# Assess the total accuracy of test data
ct4.mda <- table(test$Order, mda4.predict)
totcorrect4.mda <- sum(diag(prop.table(ct4.mda)))

#mda - 5 component 
mda.5 <- mda(Order ~ pca.PC1 + pca.PC2+ pca.PC3+ pca.PC4 + pca.PC5, data = train)
#predict test data
mda5.predict = predict(mda.5, newdata=test)
# Assess the total accuracy of test data
ct5.mda <- table(test$Order, mda5.predict)
totcorrect5.mda <- sum(diag(prop.table(ct5.mda)))

#mda - 6 component 
mda.6 <- mda(Order ~ pca.PC1 + pca.PC2+ pca.PC3+ pca.PC4 + pca.PC5+ pca.PC6, data = train)
#predict test data
mda6.predict = predict(mda.6, newdata=test)
# Assess the total accuracy of test data
ct6.mda <- table(test$Order, mda6.predict)
totcorrect6.mda <- sum(diag(prop.table(ct6.mda)))

#mda - 7 component 
mda.7 <- mda(Order ~ pca.PC1 + pca.PC2+ pca.PC3+ pca.PC4 + pca.PC5+ pca.PC6+ pca.PC7, data = train)
#predict test data
mda7.predict = predict(mda.7, newdata=test)
# Assess the total accuracy of test data
ct7.mda <- table(test$Order, mda7.predict)
totcorrect7.mda <- sum(diag(prop.table(ct7.mda)))

#mda - 8 component 
mda.8 <- mda(Order ~ pca.PC1 + pca.PC2+ pca.PC3+ pca.PC4 + pca.PC5+ pca.PC6+ pca.PC7+ pca.PC8, data = train)
#predict test data
mda8.predict = predict(mda.8, newdata=test)
# Assess the total accuracy of test data
ct8.mda <- table(test$Order, mda8.predict)
totcorrect8.mda <- sum(diag(prop.table(ct8.mda)))

```

In this part of the analysis, we randomly divided our dataset to traning data (80%) and test data (20%). We applied MDA in training data to get the model and then evaluated its accuracy in test data. In order to test the effects of the number of components in model performance and elaborate the dilemma between model complexity and prediction accuracy, we conducted MDA with the number of components included ranging from 2 to 8. The plot below shows the increasing prediction accuracy with the increasing number of components included. Along with the number of components increasing from 2 to 8, the predicting accuracy increased from `r round(totcorrect2.mda, 2) * 100`% to `r round(totcorrect8.mda, 2) * 100`%. From a perspective of balancing model complexity and prediction accuracy, we chose our final MDA model with four components included and used that model to conduct following comparison with QDA model. 


## Comaprison between MDA and Quadratic Discriminant Analysis (QDA) 

```{r MDA VS QDA without PCA- Fei, echo =F}

#mda - 4 component 
mda <- mda(Order ~ pca.PC1 + pca.PC2+ pca.PC3+ pca.PC4 , data = train)
#predict test data
mda.predict = predict(mda, newdata=test)
# Assess the accuracy of the prediction for each soil class (Order) in test data
ct.mda <- table(test$Order, mda.predict)
cc.mda <- diag(prop.table(ct.mda, 1))
# Assess the total accuracy of test data
totcorrect.mda <- sum(diag(prop.table(ct4.mda)))


#qda
qda.fei <- MASS::qda(Order ~pca.PC1 + pca.PC2+ pca.PC3+ pca.PC4, data = train)

#predict test data
qda.predict = predict(qda.fei, newdata=test)

# Assess the accuracy of the prediction for each soil class (Order) in test data
ct.qda <- table(test$Order, qda.predict$class)
cc.qda <- diag(prop.table(ct.qda, 1))


### comparison 
#mda
correct.mda <- diag(prop.table(ct.mda, 1))
# total percent correct
totcorrect.mda <- sum(diag(prop.table(ct.mda)))

#qda
correct.qda <- diag(prop.table(ct.qda, 1))
# total percent correct
totcorrect.qda <- sum(diag(prop.table(ct.qda)))

```

In this analysis, we conducted MDA and QDA analysis based on the same training and test dataset and the same first four components. The prediction accuracy for each soil types and overall accuracy of these two methods are shown in the table below. For the table we can see that the overall accuracy of MDA and QDA are very similar (`r round(totcorrect.mda, 2) * 100`% for MDA and `r round(totcorrect.qda, 2) * 100`% for QDA). However, their ability in correctly predicting individual soil groups differed. For instance, MDA is better at Inceptisols and Mollisols while QDA is better at Aridisols and Vertisols. 

```{r Compare MDA and QDA}

##table 

model_comparison <- data.frame(rbind(correct.mda, correct.qda))
model_comparison$Overall=c(totcorrect.mda,totcorrect.qda)

model_comparison <-model_comparison %>%set_rownames(c("MDA", "QDA"))
knitr::kable(round(model_comparison, digits = 2))

#p <- ggplot(test, aes(x = Sand, y = Clay, color = mda.predict)) + geom_point() + stat_contour(aes(x = Sand, y = Clay, z = mda.predict), data = test) + labs(title="MDA Decision Boundaries")
#p
```

#Conclusion

In this project, we finished up two part. 

In the first part, 

In the second part, we conducted MDA on our dataset and explored the increasing prediction accuracy with the increasing model complexity. Through comparison with QDA on the same dataset using same predictors, we found out the comparably acceptable performance of these two methods. 


#Contributions

The different tasks required to complete this project were equally divided between Meridith and Fei. Both members of this group contributed to the presentation slides and this report. 
  