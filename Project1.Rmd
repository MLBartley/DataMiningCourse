---
title: "Project 1"
output: html_notebook
---


#Introduction

This is Project 1 for STAT 557 2018 Spring by Meridith Bartley and Fei. The aim of this project is to practice linear classification methods and QDA and study basic techniques of dimension reduction.In this project we (1) apply LDA, QDA, and multinomial logistic regression to soil sample data in order to calssify into separate soil group (Orders). 

#Description of Data
This is Fei's research data. It's about soil sample data over the US downloaded from NRCS, including the physical and chemical properties of soil samples (sand, silt, clay, oganic carbon, bulk  density, CEC soil, CEC clay, base satuaration, and pH) and the corresponding soil classification group (soil order). 
 


```{r load code and clean, include=FALSE, echo = F, message = F}

project1data <- read.csv(file = "project1data.csv")
project1data <- project1data[,-c(1,2,4)] #remove the ID and Horizon columns which are useless,Silt is redundant information since it could be calculated from Sand and Clay. 
project1data <- project1data[sample(nrow(project1data),nrow(project1data)),] ###shuffle rows

library(ggplot2)
library(dplyr)
library(magrittr)
library(MASS)
library(caret)
library(nnet) 


```


```{r EDA - Mer, echo=FALSE, message = F, results = 'hide', fig.keep='all'}

varlist <- names(project1data)[-9]

customPlot <- function(varName) {

project1data %>% 
group_by_("Order") %>% 
select_("Order",varName) %>% 
ggplot(aes_string("Order",varName)) + geom_boxplot()

}

lapply(varlist,customPlot)


```


#Analysis

```{r LDA - Fei}



#data partition
lda_train <- caret::createDataPartition(y=project1data$Order, p=0.8,list = FALSE)
train <- project1data[lda_train,]
test <- project1data[-lda_train,]

#lda
lda.fei <- MASS::lda(Order ~., data = project1data, subset =lda_train)

#predict test data
lda.predict = predict(lda.fei, newdata=test)

# Assess the accuracy of the prediction percent correct for each soil class (Order) in test data
ct.lda <- table(test$Order, lda.predict$class)
diag(prop.table(ct.lda, 1))

# total percent correct
sum(diag(prop.table(ct.lda)))

```


```{r QDA - Fei}

#data partition
qda_train <- createDataPartition(y=project1data$Order, p=0.8,list = FALSE)
train <- project1data[qda_train,]
test <- project1data[-qda_train,]

#qda
qda.fei <- MASS::qda(Order ~., data = project1data, subset =qda_train)

#predict test data
qda.predict = predict(qda.fei, newdata=test)

# Assess the accuracy of the prediction percent correct for each soil class (Order) in test data
ct.qda <- table(test$Order, qda.predict$class)
diag(prop.table(ct.qda, 1))

# total percent correct
sum(diag(prop.table(ct.qda)))

```

```{r Log Regression - Mer}

#data partition
logr_train <- caret::createDataPartition(y=project1data$Order, p=0.8,list = FALSE)
train <- project1data[logr_train,]
test <- project1data[-logr_train,]

#doing logistic regression

# mylogr <- function(y, x, maxit = 100, tol = 1e-5) {
#   y <- as.numeric(y) - 1
#   X <- as.matrix(cbind(1, x))      # add the intercept term to the predictors
#   
#   # constants
#   N <- length(y)        # number of observations
#   p <- ncol(X)          # number of predictors
#   continue <- T
#   i <- 1
#   
#   betas <- matrix(0, nrow = maxit, ncol = p)
#   
#   #below is from PseudoCode - slide 21 on log reg lecture
#   while(continue && i < maxit) {
#     beta <- betas[i, ]
#     p <- exp(X %*% beta)/(1 + exp(X %*% beta))
#     W <- diag(as.vector(p * (1-p)))
#     z <- X %*% beta + solve(W) %*% (y - p)
#     betas[i+1, ] <- solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% z
#     if(all(abs(betas[i + 1, ] - betas[i, ])/abs(betas[i, ]) < tol)) {
#       continue <- F
#     }
#     i <- i + 1
#   }
#   
#   return(betas[i, ])
#   
# }


#cross validation
# mylogr.cv <- function(y, x, k, maxit = 100, tol = 1e-5) {
#   
#   cvs <- rep(0, k)
#   
#   # constants
#   N <- length(y)        # number of observations
#   
#   # assign indices for which group each observation belongs to
#   kappa <- sample(rep(1:k, length = N))
#   
#   for(i in 1:k) {
#     y.tr <- y[kappa != i]     # training responses; all but ith group
#     x.tr <- x[kappa != i, ]   # training predictors; all but ith group
#     y.va <- y[kappa == i]     # validation responses; ith group
#     x.va <- x[kappa == i, ]   # validation predictors; ith group
#     
#     beta <- mylogr(y.tr, x.tr, maxit, tol)
#     
#     X.va <- as.matrix(cbind(1, x.va))
#     
#     prediction <- rep(NA, length(y.va))
#     for(j in 1:length(prediction)) {
#       prediction[j] <- round(1/(1+exp(-t(beta) %*% X.va[j, ])))
#     }
#     cvs[i] <- sum((prediction - (as.numeric(y.va)-1))^2)
#   }
#   
#   return(sum(cvs)/N)
#   
# }

# system.time(cv.err.logr <- mylogr.cv(data$class, data[ ,-10], 10))
  
  
#below doesn't seem to work - W becomes singular
# logr.mer <- mylogr(y = train[1:100, 9] , x = train[1:100,-9])


logr.mer <- nnet::multinom(Order ~ ., data = project1data, subset = logr_train)
summary(logr.mer)
  


```

```{r Model Comparison - Mer}

```

#Results



#Contributions
  