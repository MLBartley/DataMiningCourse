---
title: "Project 1"
output:
  pdf_document: 
    keep_tex: yes
    latex_engine: xelatex
  # html_notebook: default
---


#Introduction

This is Project 1 for STAT 557 2018 Spring by Meridith Bartley and Fei. The aim of this project is to practice linear classification methods and QDA and study basic techniques of dimension reduction.In this project we (1) apply LDA, QDA, and multinomial logistic regression to soil sample data in order to calssify into separate soil group (Orders). 

#Description of Data
This is Fei's research data. It's about soil sample data over the US downloaded from NRCS, including the physical and chemical properties of soil samples (sand, silt, clay, oganic carbon, bulk  density, CEC soil, CEC clay, base satuaration, and pH) and the corresponding soil classification group (soil order). 

Boxplots for each physical and chemical property used as explanitory variables in the subsequent classification models are included below. This EDA allows for early indication of which variables may possibly be ommitted during dimention reduction. That is, what properties do not differ significantly between soil Orders.

```{r load code and clean, include=FALSE, echo = F, message = F}

project1data <- read.csv(file = "project1data.csv")
project1data <- project1data[,-c(1,2,4)] #remove the ID and Horizon columns which are useless,Silt is redundant information since it could be calculated from Sand and Clay. 
project1data <- project1data[sample(nrow(project1data),nrow(project1data)),] ###shuffle rows

library(ggplot2)
library(dplyr)
library(magrittr)
library(MASS)
library(caret)
library(nnet) 
library(scales)
library(klaR)


```

## Exploritory Data Analysis
```{r EDA - Mer, echo=FALSE, message = F, results = 'hide', fig.keep='all'}

varlist <- names(project1data)[-9]

customPlot <- function(varName) {

project1data %>% 
group_by_("Order") %>% 
select_("Order",varName) %>% 
ggplot(aes_string("Order",varName)) + geom_boxplot()

}

lapply(varlist,customPlot)

pairs(project1data)
cor(project1data[, -9])


```


#Analysis

##Linear Discriminant Analysis (LDA)


```{r LDA - Fei, echo=FALSE, fig.keep = 'all'}



#data partition
lda_train <- caret::createDataPartition(y=project1data$Order, p=0.8,list = FALSE)
train <- project1data[lda_train,]
test <- project1data[-lda_train,]

#lda
lda.fei <- MASS::lda(Order ~., data = project1data, subset =lda_train)
# plot(lda.fei)
#predict test data
lda.predict = predict(lda.fei, newdata=test)

# Assess the accuracy of the prediction percent correct for each soil class (Order) in test data
ct.lda <- table(test$Order, lda.predict$class)
diag(prop.table(ct.lda, 1))

# total percent correct
sum(diag(prop.table(ct.lda)))



# Visualizing Data separation for chosen variables

 prop.lda = lda.fei$svd^2/sum(lda.fei$svd^2)

dataset = data.frame(Order = test$Order,
                     lda = lda.predict$x)

lda1 <- ggplot(dataset) + geom_point(aes(lda.LD1, lda.LD2, colour = Order), size = 1) +
  labs(x = paste("LD1 (", percent(prop.lda[1]), ")", sep=""),
       y = paste("LD2 (", percent(prop.lda[2]), ")", sep=""))


print(lda1)


# plot results 
plot.lda = data.frame(Pclass=lda.predict$class,lda=lda.predict$x)
p.lda <- ggplot(plot.lda)+ geom_point(aes(lda.LD1,lda.LD2,colour=Pclass),size=1)
p.lda

## MER: are LD1 and LD2 Sand + Clay? 
```

## Quadratic Discriminant Analysis

```{r QDA - Fei, echo =F}

#data partition
qda_train <- createDataPartition(y=project1data$Order, p=0.8,list = FALSE)
train <- project1data[qda_train,]
test <- project1data[-qda_train,]

#qda
qda.fei <- MASS::qda(Order ~., data = project1data, subset =qda_train)

#predict test data
qda.predict = predict(qda.fei, newdata=test)

# Assess the accuracy of the prediction percent correct for each soil class (Order) in test data
ct.qda <- table(test$Order, qda.predict$class)
diag(prop.table(ct.qda, 1))

# total percent correct
sum(diag(prop.table(ct.qda)))

# plot results # 
# partimat(Order ~., data = project1data,metod='qda') figures are too large and took forever to show up. 

```

##Multinomial Logistic Regression

```{r Log Regression - Mer, echo = F, results = 'hide'}

#data partition
logr_train <- caret::createDataPartition(y=project1data$Order, p=0.8,list = FALSE)
train <- project1data[logr_train,]
test <- project1data[-logr_train,]

#doing logistic regression

# mylogr <- function(y, x, maxit = 100, tol = 1e-5) {
#   y <- as.numeric(y) - 1
#   X <- as.matrix(cbind(1, x))      # add the intercept term to the predictors
#   
#   # constants
#   N <- length(y)        # number of observations
#   p <- ncol(X)          # number of predictors
#   continue <- T
#   i <- 1
#   
#   betas <- matrix(0, nrow = maxit, ncol = p)
#   
#   #below is from PseudoCode - slide 21 on log reg lecture
#   while(continue && i < maxit) {
#     beta <- betas[i, ]
#     p <- exp(X %*% beta)/(1 + exp(X %*% beta))
#     W <- diag(as.vector(p * (1-p)))
#     z <- X %*% beta + solve(W) %*% (y - p)
#     betas[i+1, ] <- solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% z
#     if(all(abs(betas[i + 1, ] - betas[i, ])/abs(betas[i, ]) < tol)) {
#       continue <- F
#     }
#     i <- i + 1
#   }
#   
#   return(betas[i, ])
#   
# }


#cross validation
# mylogr.cv <- function(y, x, k, maxit = 100, tol = 1e-5) {
#   
#   cvs <- rep(0, k)
#   
#   # constants
#   N <- length(y)        # number of observations
#   
#   # assign indices for which group each observation belongs to
#   kappa <- sample(rep(1:k, length = N))
#   
#   for(i in 1:k) {
#     y.tr <- y[kappa != i]     # training responses; all but ith group
#     x.tr <- x[kappa != i, ]   # training predictors; all but ith group
#     y.va <- y[kappa == i]     # validation responses; ith group
#     x.va <- x[kappa == i, ]   # validation predictors; ith group
#     
#     beta <- mylogr(y.tr, x.tr, maxit, tol)
#     
#     X.va <- as.matrix(cbind(1, x.va))
#     
#     prediction <- rep(NA, length(y.va))
#     for(j in 1:length(prediction)) {
#       prediction[j] <- round(1/(1+exp(-t(beta) %*% X.va[j, ])))
#     }
#     cvs[i] <- sum((prediction - (as.numeric(y.va)-1))^2)
#   }
#   
#   return(sum(cvs)/N)
#   
# }

# system.time(cv.err.logr <- mylogr.cv(data$class, data[ ,-10], 10))
  
  
#below doesn't seem to work - W becomes singular
# logr.mer <- mylogr(y = train[1:100, 9] , x = train[1:100,-9])


logr.mer <- nnet::multinom(Order ~ ., data = project1data, subset = logr_train)
summary(logr.mer)

logr.predict <- predict(logr.mer, newdata = test)
  


```



#Results

The results from these three approaches show that...


In order to compare the reults it is important to recall the diffences between these three classification approaches. The difference between LDA and logistic regression is that linear coefficients are estimated differently. MLE for logistic models and estimated mean and variance based on Gaussian assumptions for the LDA. LDA makes more restrictive Gaussian assumptions and therefore often expected to work better than logistic models IF they are met. QDA serves as a compromise between non-parametric methods (not explored in this project) and the linear LDA and logistic regression approaches. Since QDA assumes a quadratic decision boundary, it can accurately model a wider range of problems than can the linear methods. QDA can perform better in the presence of a limited number of training observations because it does make some assumptions about the form of the decision boundary.


```{r Model Comparison - Mer}



```

#Contributions
  