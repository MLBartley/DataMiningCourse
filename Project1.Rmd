---
title: "Project 1"
output: html_notebook
---


#Introduction
This is Project 1 for STAT557 2018Spring by Meridith and Fei. It is about LDA, QDA and Log Regression.

#Description of Data
This is Fei's research data. It's about soil sample data over the US downloaded from NRCS, including the physical and chemical properties of soil samples (sand, silt, clay, oganic carbon, bulk  density, CEC soil, CEC clay, base satuaration, and pH) and the corresponding soil classification group (soil order). 
 


```{r load code and clean, include=FALSE}

project1data <- read.csv(file = "project1data.csv")
project1data <- project1data[,-c(1,2,4)] #remove the ID and Horizon columns which are useless,Silt is redundant information since it could be calculated from Sand and Clay. 
project1data <- project1data[sample(nrow(project1data),nrow(project1data)),] ###shuffle rows
```


#Analysis

```{r LDA - Fei}

library(MASS)
library(caret)

#data partition
lda_train <- createDataPartition(y=project1data$Order, p=0.8,list = FALSE)
train <- project1data[lda_train,]
test <- project1data[-lda_train,]

#lda
lda.fei <- lda(Order ~., data = project1data, subset =lda_train)

#predict test data
lda.predict = predict(lda.fei, newdata=test)

# Assess the accuracy of the prediction percent correct for each soil class (Order) in test data
ct.lda <- table(test$Order, lda.predict$class)
diag(prop.table(ct.lda, 1))

# total percent correct
sum(diag(prop.table(ct.lda)))

```


```{r QDA - Fei}
library(MASS)
library(caret)

#data partition
qda_train <- createDataPartition(y=project1data$Order, p=0.8,list = FALSE)
train <- project1data[qda_train,]
test <- project1data[-qda_train,]

#qda
qda.fei <- qda(Order ~., data = project1data, subset =qda_train)

#predict test data
qda.predict = predict(qda.fei, newdata=test)

# Assess the accuracy of the prediction percent correct for each soil class (Order) in test data
ct.qda <- table(test$Order, qda.predict$class)
diag(prop.table(ct.qda, 1))

# total percent correct
sum(diag(prop.table(ct.qda)))

```

```{r Log Regression - Mer}

#doing logistic regression

mylogr <- function(y, x, maxit = 100, tol = 1e-5) {
  y <- as.numeric(y) - 1
  X <- as.matrix(cbind(1, x))      # add the intercept term to the predictors
  
  # constants
  N <- length(y)        # number of observations
  p <- ncol(X)          # number of predictors
  continue <- T
  i <- 1
  
  betas <- matrix(0, nrow = maxit, ncol = p)
  
  #below is from PseudoCode - slide 21 on log reg lecture
  while(continue && i < maxit) {
    beta <- betas[i, ]
    p <- exp(X %*% beta)/(1 + exp(X %*% beta))
    W <- diag(as.vector(p * (1-p)))
    z <- X %*% beta + solve(W) %*% (y - p)
    betas[i+1, ] <- solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% z
    if(all(abs(betas[i + 1, ] - betas[i, ])/abs(betas[i, ]) < tol)) {
      continue <- F
    }
    i <- i + 1
  }
  
  return(betas[i, ])
  
}


#cross validation
mylogr.cv <- function(y, x, k, maxit = 100, tol = 1e-5) {
  
  cvs <- rep(0, k)
  
  # constants
  N <- length(y)        # number of observations
  
  # assign indices for which group each observation belongs to
  kappa <- sample(rep(1:k, length = N))
  
  for(i in 1:k) {
    y.tr <- y[kappa != i]     # training responses; all but ith group
    x.tr <- x[kappa != i, ]   # training predictors; all but ith group
    y.va <- y[kappa == i]     # validation responses; ith group
    x.va <- x[kappa == i, ]   # validation predictors; ith group
    
    beta <- mylogr(y.tr, x.tr, maxit, tol)
    
    X.va <- as.matrix(cbind(1, x.va))
    
    prediction <- rep(NA, length(y.va))
    for(j in 1:length(prediction)) {
      prediction[j] <- round(1/(1+exp(-t(beta) %*% X.va[j, ])))
    }
    cvs[i] <- sum((prediction - (as.numeric(y.va)-1))^2)
  }
  
  return(sum(cvs)/N)
  
}

system.time(cv.err.logr <- mylogr.cv(data$class, data[ ,-10], 10))
  
  
  
```

```{r Model Comparison - Mer}

```

#Results

#Conclusions

#Contributions
  